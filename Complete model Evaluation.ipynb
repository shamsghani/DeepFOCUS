{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3853fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df436cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 1️⃣ GPU / CPU Detection\n",
    "# ============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"⚠️ No GPU detected, training will be on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ff98b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 2️⃣ Live training plot\n",
    "# ============================\n",
    "train_loss_hist = []\n",
    "val_loss_hist = []\n",
    "\n",
    "def live_plot():\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_loss_hist, label=\"Train Loss\")\n",
    "    plt.plot(val_loss_hist, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Live Training Progress\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772da628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 3️⃣ Helper Functions\n",
    "# ============================\n",
    "def project_01(im: np.ndarray) -> np.ndarray:\n",
    "    im = np.squeeze(im)\n",
    "    min_val = im.min()\n",
    "    max_val = im.max()\n",
    "    return (im - min_val) / (max_val - min_val)\n",
    "\n",
    "def normalize_im(im: np.ndarray, dmean: float, dstd: float) -> np.ndarray:\n",
    "    im = np.squeeze(im)\n",
    "    return (im - dmean) / dstd\n",
    "\n",
    "def matlab_style_gauss2D(shape=(7, 7), sigma=1) -> np.ndarray:\n",
    "    m, n = [(ss - 1.) / 2. for ss in shape]\n",
    "    y, x = np.ogrid[-m:m+1, -n:n+1]\n",
    "    h = np.exp(-(x*x + y*y) / (2. * sigma * sigma))\n",
    "    h[h < np.finfo(h.dtype).eps * h.max()] = 0\n",
    "    sumh = h.sum()\n",
    "    if sumh != 0:\n",
    "        h /= sumh\n",
    "    h *= 2.0\n",
    "    return h.astype(np.float32)\n",
    "\n",
    "# Gaussian PSF filter\n",
    "psf_heatmap = matlab_style_gauss2D(shape=(7, 7), sigma=1)\n",
    "gfilter = torch.tensor(psf_heatmap, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5236800",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# 4️⃣ Custom Loss (L1 + L2)\n",
    "# ============================\n",
    "class L1L2Loss(nn.Module):\n",
    "    def __init__(self, gfilter):\n",
    "        super().__init__()\n",
    "        self.gfilter = gfilter\n",
    "\n",
    "    def forward(self, heatmap_true, spikes_pred):\n",
    "        heatmap_pred = F.conv2d(spikes_pred, self.gfilter, padding=3)\n",
    "        loss_heatmaps = F.mse_loss(heatmap_true, heatmap_pred)\n",
    "        loss_spikes = torch.mean(torch.abs(spikes_pred))\n",
    "        return loss_heatmaps + loss_spikes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f624ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.75, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = self.bce(inputs, targets)\n",
    "        probs = torch.sigmoid(inputs)\n",
    "        pt = torch.where(targets == 1, probs, 1 - probs)\n",
    "        focal = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "        return focal.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb93c64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 5️⃣ CNN Architecture\n",
    "# ============================\n",
    "class ConvBNReLU(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=3):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=k, padding=1, bias=False)\n",
    "        nn.init.orthogonal_(self.conv.weight)\n",
    "        self.bn = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86803f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBNReLU(1, 32)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = ConvBNReLU(32, 64)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.enc3 = ConvBNReLU(64, 128)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.enc4 = ConvBNReLU(128, 512)\n",
    "\n",
    "        self.up1 = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "        self.dec1 = ConvBNReLU(512, 128)\n",
    "        self.up2 = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "        self.dec2 = ConvBNReLU(128, 64)\n",
    "        self.up3 = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "        self.dec3 = ConvBNReLU(64, 32)\n",
    "\n",
    "        self.pred = nn.Conv2d(32, 1, kernel_size=1, bias=False)\n",
    "        nn.init.orthogonal_(self.pred.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.enc1(x)\n",
    "        x2 = self.enc2(self.pool1(x1))\n",
    "        x3 = self.enc3(self.pool2(x2))\n",
    "        x4 = self.enc4(self.pool3(x3))\n",
    "\n",
    "        x = self.up1(x4)\n",
    "        x = self.dec1(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.dec2(x)\n",
    "        x = self.up3(x)\n",
    "        x = self.dec3(x)\n",
    "        return self.pred(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76da46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# 6️⃣ Load & preprocess data\n",
    "# ============================\n",
    "with open(\"from scratch_training_data.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "patches = data[\"patches\"]\n",
    "heatmaps = data[\"heatmaps\"]\n",
    "\n",
    "print(\"✅ Loaded data shapes:\")\n",
    "print(\"patches:\", patches.shape)\n",
    "print(\"heatmaps:\", heatmaps.shape)\n",
    "\n",
    "# Normalization\n",
    "mean_val = np.mean(patches)\n",
    "std_val = np.std(patches)\n",
    "patches = (patches - mean_val) / std_val\n",
    "\n",
    "# Add channel axis\n",
    "patches = patches[..., np.newaxis]\n",
    "heatmaps = heatmaps[..., np.newaxis]\n",
    "\n",
    "# Train/val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    patches, heatmaps, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).permute(0, 3, 1, 2).to(device)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).permute(0, 3, 1, 2).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).permute(0, 3, 1, 2).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).permute(0, 3, 1, 2).to(device)\n",
    "\n",
    "print(f\"✅ Train: {X_train.shape}, Val: {X_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd37460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Train to overfit on mini-batch\n",
    "# ============================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Choose a small random subset (100 samples)\n",
    "micro_size = 100\n",
    "indices = np.random.choice(len(patches), size=micro_size, replace=False)\n",
    "\n",
    "patches_micro = patches[indices]\n",
    "heatmaps_micro = heatmaps[indices]\n",
    "\n",
    "print(\"🎯 Microsample set created:\")\n",
    "print(\"patches_micro:\", patches_micro.shape)\n",
    "print(\"heatmaps_micro:\", heatmaps_micro.shape)\n",
    "\n",
    "# Train/val split (e.g., 80/20)\n",
    "X_train_micro, X_val_micro, y_train_micro, y_val_micro = train_test_split(\n",
    "    patches_micro, heatmaps_micro, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train_micro = torch.tensor(X_train_micro, dtype=torch.float32).permute(0, 3, 1, 2).to(device)\n",
    "X_val_micro   = torch.tensor(X_val_micro, dtype=torch.float32).permute(0, 3, 1, 2).to(device)\n",
    "y_train_micro = torch.tensor(y_train_micro, dtype=torch.float32).permute(0, 3, 1, 2).to(device)\n",
    "y_val_micro   = torch.tensor(y_val_micro, dtype=torch.float32).permute(0, 3, 1, 2).to(device)\n",
    "\n",
    "print(f\"✅ Micro Train: {X_train_micro.shape}, Micro Val: {X_val_micro.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ae36fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Sanity chgeck: Visualize patches and heatmaps\n",
    "# ============================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Pick 10 random indices\n",
    "idxs = np.random.choice(len(X_train), size=10, replace=False)\n",
    "\n",
    "plt.figure(figsize=(12, 30))\n",
    "\n",
    "for i, idx in enumerate(idxs):\n",
    "    patch = X_train[idx].cpu().numpy().squeeze()\n",
    "    heatmap = y_train[idx].cpu().numpy().squeeze()\n",
    "\n",
    "    # Normalize for display\n",
    "    patch_disp = (patch - patch.min()) / (patch.max() - patch.min() + 1e-8)\n",
    "    heatmap_disp = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)\n",
    "\n",
    "    # Overlay (red = GT)\n",
    "    overlay = np.stack([\n",
    "        heatmap_disp,                # Red channel\n",
    "        patch_disp * 0.7,            # Green channel\n",
    "        patch_disp * 0.7,            # Blue channel\n",
    "    ], axis=-1)\n",
    "\n",
    "    # Plot patch\n",
    "    plt.subplot(10, 3, 3*i + 1)\n",
    "    plt.imshow(patch_disp, cmap=\"gray\")\n",
    "    plt.title(\"Patch\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Plot GT heatmap\n",
    "    plt.subplot(10, 3, 3*i + 2)\n",
    "    plt.imshow(heatmap_disp, cmap=\"hot\")\n",
    "    plt.title(\"GT Heatmap\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Plot overlay\n",
    "    plt.subplot(10, 3, 3*i + 3)\n",
    "    plt.imshow(overlay)\n",
    "    plt.title(\"Overlay\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9280ab",
   "metadata": {},
   "source": [
    "ground truth matches up with corresponding patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ae5376",
   "metadata": {},
   "source": [
    "Original Structure, L1+L2 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f6b43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_idx = random.randint(0, X_val.shape[0]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687c952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08916882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 7️⃣ Training setup\n",
    "# ============================\n",
    "model_L1L2_old = CNNModel().to(device)\n",
    "criterion = L1L2Loss(gfilter)\n",
    "\n",
    "optimizer = optim.Adam(model_L1L2_old.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec8e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Training loop (Microsample)\n",
    "# ============================\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 8\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_L1L2_old.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # Training loop with tqdm\n",
    "    train_iter = tqdm(range(0, len(X_train_micro), batch_size), desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n",
    "    for i in train_iter:\n",
    "        xb = X_train_micro[i:i+batch_size]\n",
    "        yb = y_train_micro[i:i+batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_L1L2_old(xb)\n",
    "        loss = criterion(yb, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        avg_so_far = train_loss / ((i // batch_size) + 1)\n",
    "        train_iter.set_postfix(loss=avg_so_far)\n",
    "\n",
    "    # Validation loop with tqdm\n",
    "    model_L1L2_old.eval()\n",
    "    val_loss = 0.0\n",
    "    val_iter = tqdm(range(0, len(X_val_micro), batch_size), desc=f\"Epoch {epoch+1}/{epochs} [Val]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for i in val_iter:\n",
    "            xb = X_val_micro[i:i+batch_size]\n",
    "            yb = y_val_micro[i:i+batch_size]\n",
    "            outputs = model_L1L2_old(xb)\n",
    "            loss = criterion(yb, outputs)\n",
    "            val_loss += loss.item()\n",
    "            avg_so_far = val_loss / ((i // batch_size) + 1)\n",
    "            val_iter.set_postfix(loss=avg_so_far)\n",
    "\n",
    "    avg_train_loss = train_loss / (len(X_train_micro) / batch_size)\n",
    "    avg_val_loss = val_loss / (len(X_val_micro) / batch_size)\n",
    "\n",
    "    train_loss_hist.append(avg_train_loss)\n",
    "    val_loss_hist.append(avg_val_loss)\n",
    "    live_plot()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5bf4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# 9️⃣ Prediction example\n",
    "# ============================\n",
    "# i = random.randint(0, X_val.shape[0]-1)\n",
    "input_patch = X_val[sample_idx].cpu().squeeze().numpy()\n",
    "true_heatmap = y_val[sample_idx].cpu().squeeze().numpy()\n",
    "\n",
    "model_L1L2_old.eval()\n",
    "with torch.no_grad():\n",
    "    predicted = model_L1L2_old(X_val[sample_idx].unsqueeze(0)).cpu().squeeze().numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "# plt.subplot(1, 3, 1); plt.imshow(input_patch, cmap='gray'); plt.title(\"Input Patch\")\n",
    "# plt.subplot(1, 3, 2); plt.imshow(true_heatmap, cmap='hot'); plt.title(\"True Heatmap\")\n",
    "plt.subplot(1, 3, 3); plt.imshow(predicted, cmap='hot'); plt.title(\"Predicted Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27e0f93",
   "metadata": {},
   "source": [
    "terrible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2737c7bf",
   "metadata": {},
   "source": [
    "Original Structure, MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cf9ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 7️⃣ Training setup\n",
    "# ============================\n",
    "model_MSE_old = CNNModel().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_MSE_old.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eb7c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Training loop (Microsample)\n",
    "# ============================\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 8\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_MSE_old.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # Training loop with tqdm\n",
    "    train_iter = tqdm(range(0, len(X_train_micro), batch_size), desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n",
    "    for i in train_iter:\n",
    "        xb = X_train_micro[i:i+batch_size]\n",
    "        yb = y_train_micro[i:i+batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_MSE_old(xb)\n",
    "        loss = criterion(yb, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        avg_so_far = train_loss / ((i // batch_size) + 1)\n",
    "        train_iter.set_postfix(loss=avg_so_far)\n",
    "\n",
    "    # Validation loop with tqdm\n",
    "    model_MSE_old.eval()\n",
    "    val_loss = 0.0\n",
    "    val_iter = tqdm(range(0, len(X_val_micro), batch_size), desc=f\"Epoch {epoch+1}/{epochs} [Val]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for i in val_iter:\n",
    "            xb = X_val_micro[i:i+batch_size]\n",
    "            yb = y_val_micro[i:i+batch_size]\n",
    "            outputs = model_MSE_old(xb)\n",
    "            loss = criterion(yb, outputs)\n",
    "            val_loss += loss.item()\n",
    "            avg_so_far = val_loss / ((i // batch_size) + 1)\n",
    "            val_iter.set_postfix(loss=avg_so_far)\n",
    "\n",
    "    avg_train_loss = train_loss / (len(X_train_micro) / batch_size)\n",
    "    avg_val_loss = val_loss / (len(X_val_micro) / batch_size)\n",
    "\n",
    "    train_loss_hist.append(avg_train_loss)\n",
    "    val_loss_hist.append(avg_val_loss)\n",
    "    live_plot()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869b8fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# 9️⃣ Prediction example\n",
    "# ============================\n",
    "# i = random.randint(0, X_val.shape[0]-1)\n",
    "input_patch = X_val[sample_idx].cpu().squeeze().numpy()\n",
    "true_heatmap = y_val[sample_idx].cpu().squeeze().numpy()\n",
    "\n",
    "model_MSE_old.eval()\n",
    "with torch.no_grad():\n",
    "    predicted = model_MSE_old(X_val[sample_idx].unsqueeze(0)).cpu().squeeze().numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "# plt.subplot(1, 3, 1); plt.imshow(input_patch, cmap='gray'); plt.title(\"Input Patch\")\n",
    "# plt.subplot(1, 3, 2); plt.imshow(true_heatmap, cmap='hot'); plt.title(\"True Heatmap\")\n",
    "plt.subplot(1, 3, 3); plt.imshow(predicted, cmap='hot'); plt.title(\"Predicted Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b5e32d",
   "metadata": {},
   "source": [
    "terrible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08072ddc",
   "metadata": {},
   "source": [
    "Original Structure, BCE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d73e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 7️⃣ Training setup\n",
    "# ============================\n",
    "model_BCE_old = CNNModel().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([20.0], device=device))\n",
    "optimizer = optim.Adam(model_BCE_old.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dac6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Training loop (Microsample)\n",
    "# ============================\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 8\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_BCE_old.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # Training loop with tqdm\n",
    "    train_iter = tqdm(range(0, len(X_train_micro), batch_size), desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n",
    "    for i in train_iter:\n",
    "        xb = X_train_micro[i:i+batch_size]\n",
    "        yb = y_train_micro[i:i+batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_BCE_old(xb)\n",
    "        loss = criterion(outputs, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        avg_so_far = train_loss / ((i // batch_size) + 1)\n",
    "        train_iter.set_postfix(loss=avg_so_far)\n",
    "\n",
    "    # Validation loop with tqdm\n",
    "    model_BCE_old.eval()\n",
    "    val_loss = 0.0\n",
    "    val_iter = tqdm(range(0, len(X_val_micro), batch_size), desc=f\"Epoch {epoch+1}/{epochs} [Val]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for i in val_iter:\n",
    "            xb = X_val_micro[i:i+batch_size]\n",
    "            yb = y_val_micro[i:i+batch_size]\n",
    "            outputs = model_BCE_old(xb)\n",
    "            loss = criterion(outputs, yb)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            avg_so_far = val_loss / ((i // batch_size) + 1)\n",
    "            val_iter.set_postfix(loss=avg_so_far)\n",
    "\n",
    "    avg_train_loss = train_loss / (len(X_train_micro) / batch_size)\n",
    "    avg_val_loss = val_loss / (len(X_val_micro) / batch_size)\n",
    "\n",
    "    train_loss_hist.append(avg_train_loss)\n",
    "    val_loss_hist.append(avg_val_loss)\n",
    "    live_plot()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6a212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# 9️⃣ Prediction example\n",
    "# ============================\n",
    "# i = random.randint(0, X_val.shape[0]-1)\n",
    "input_patch = X_val[sample_idx].cpu().squeeze().numpy()\n",
    "true_heatmap = y_val[sample_idx].cpu().squeeze().numpy()\n",
    "\n",
    "model_BCE_old.eval()\n",
    "with torch.no_grad():\n",
    "    predicted = model_BCE_old(X_val[sample_idx].unsqueeze(0)).cpu().squeeze().numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "# plt.subplot(1, 3, 1); plt.imshow(input_patch, cmap='gray'); plt.title(\"Input Patch\")\n",
    "# plt.subplot(1, 3, 2); plt.imshow(true_heatmap, cmap='hot'); plt.title(\"True Heatmap\")\n",
    "plt.subplot(1, 3, 3); plt.imshow(predicted, cmap='hot'); plt.title(\"Predicted Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0c4224",
   "metadata": {},
   "source": [
    "not the worst thing ive ever seen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83750cb2",
   "metadata": {},
   "source": [
    "Original Structure, Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a741979",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# 7️⃣ Training setup\n",
    "# ============================\n",
    "model_focal_old = CNNModel().to(device)\n",
    "criterion = FocalLoss(alpha=0.75, gamma=2.0)\n",
    "\n",
    "optimizer = optim.Adam(model_focal_old.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc8dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Training loop (Microsample)\n",
    "# ============================\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 8\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_focal_old.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # Training loop with tqdm\n",
    "    train_iter = tqdm(range(0, len(X_train_micro), batch_size), desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n",
    "    for i in train_iter:\n",
    "        xb = X_train_micro[i:i+batch_size]\n",
    "        yb = y_train_micro[i:i+batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_focal_old(xb)\n",
    "        loss = criterion(yb, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        avg_so_far = train_loss / ((i // batch_size) + 1)\n",
    "        train_iter.set_postfix(loss=avg_so_far)\n",
    "\n",
    "    # Validation loop with tqdm\n",
    "    model_focal_old.eval()\n",
    "    val_loss = 0.0\n",
    "    val_iter = tqdm(range(0, len(X_val_micro), batch_size), desc=f\"Epoch {epoch+1}/{epochs} [Val]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for i in val_iter:\n",
    "            xb = X_val_micro[i:i+batch_size]\n",
    "            yb = y_val_micro[i:i+batch_size]\n",
    "            outputs = model_focal_old(xb)\n",
    "            loss = criterion(yb, outputs)\n",
    "            val_loss += loss.item()\n",
    "            avg_so_far = val_loss / ((i // batch_size) + 1)\n",
    "            val_iter.set_postfix(loss=avg_so_far)\n",
    "\n",
    "    avg_train_loss = train_loss / (len(X_train_micro) / batch_size)\n",
    "    avg_val_loss = val_loss / (len(X_val_micro) / batch_size)\n",
    "\n",
    "    train_loss_hist.append(avg_train_loss)\n",
    "    val_loss_hist.append(avg_val_loss)\n",
    "    live_plot()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c65ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# 9️⃣ Prediction example\n",
    "# ============================\n",
    "# i = random.randint(0, X_val.shape[0]-1)\n",
    "input_patch = X_val[sample_idx].cpu().squeeze().numpy()\n",
    "true_heatmap = y_val[sample_idx].cpu().squeeze().numpy()\n",
    "\n",
    "model_focal_old.eval()\n",
    "with torch.no_grad():\n",
    "    predicted = model_focal_old(X_val[sample_idx].unsqueeze(0)).cpu().squeeze().numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "# plt.subplot(1, 3, 1); plt.imshow(input_patch, cmap='gray'); plt.title(\"Input Patch\")\n",
    "# plt.subplot(1, 3, 2); plt.imshow(true_heatmap, cmap='hot'); plt.title(\"True Heatmap\")\n",
    "plt.subplot(1, 3, 3); plt.imshow(predicted, cmap='hot'); plt.title(\"Predicted Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0847d147",
   "metadata": {},
   "source": [
    "Original Structure shows promise only with Focal Loss, but does not improve beyond a point, even when overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed3f75",
   "metadata": {},
   "source": [
    "Update Model Architecture, add Bias and remove BatchNorm from output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33247f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 5️⃣ CNN Architecture (updated head)\n",
    "# ============================\n",
    "class CNNModel_new(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBNReLU(1, 32)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = ConvBNReLU(32, 64)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.enc3 = ConvBNReLU(64, 128)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.enc4 = ConvBNReLU(128, 512)\n",
    "\n",
    "        self.up1 = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "        self.dec1 = ConvBNReLU(512, 128)\n",
    "        self.up2 = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "        self.dec2 = ConvBNReLU(128, 64)\n",
    "        self.up3 = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "        self.dec3 = ConvBNReLU(64, 32)\n",
    "\n",
    "        # ✅ Prediction head with bias, no BatchNorm\n",
    "        self.pred = nn.Conv2d(32, 1, kernel_size=1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.enc1(x)\n",
    "        x2 = self.enc2(self.pool1(x1))\n",
    "        x3 = self.enc3(self.pool2(x2))\n",
    "        x4 = self.enc4(self.pool3(x3))\n",
    "\n",
    "        x = self.up1(x4)\n",
    "        x = self.dec1(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.dec2(x)\n",
    "        x = self.up3(x)\n",
    "        x = self.dec3(x)\n",
    "\n",
    "        return self.pred(x)   # logits (use with BCEWithLogits or plain regression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a599ee71",
   "metadata": {},
   "source": [
    "New Structure, L1+L2 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f19c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# 7️⃣ Training setup\n",
    "# ============================\n",
    "model_L1L2 = CNNModel_new().to(device)\n",
    "criterion = L1L2Loss(gfilter)\n",
    "\n",
    "optimizer = optim.Adam(model_L1L2.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e2028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Training loop (Microsample)\n",
    "# ============================\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 8\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_L1L2.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # Training loop with tqdm\n",
    "    train_iter = tqdm(range(0, len(X_train_micro), batch_size), desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n",
    "    for i in train_iter:\n",
    "        xb = X_train_micro[i:i+batch_size]\n",
    "        yb = y_train_micro[i:i+batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_L1L2(xb)\n",
    "        loss = criterion(yb, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        avg_so_far = train_loss / ((i // batch_size) + 1)\n",
    "        train_iter.set_postfix(loss=avg_so_far)\n",
    "\n",
    "    # Validation loop with tqdm\n",
    "    model_L1L2.eval()\n",
    "    val_loss = 0.0\n",
    "    val_iter = tqdm(range(0, len(X_val_micro), batch_size), desc=f\"Epoch {epoch+1}/{epochs} [Val]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for i in val_iter:\n",
    "            xb = X_val_micro[i:i+batch_size]\n",
    "            yb = y_val_micro[i:i+batch_size]\n",
    "            outputs = model_L1L2(xb)\n",
    "            loss = criterion(yb, outputs)\n",
    "            val_loss += loss.item()\n",
    "            avg_so_far = val_loss / ((i // batch_size) + 1)\n",
    "            val_iter.set_postfix(loss=avg_so_far)\n",
    "\n",
    "    avg_train_loss = train_loss / (len(X_train_micro) / batch_size)\n",
    "    avg_val_loss = val_loss / (len(X_val_micro) / batch_size)\n",
    "\n",
    "    train_loss_hist.append(avg_train_loss)\n",
    "    val_loss_hist.append(avg_val_loss)\n",
    "    live_plot()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5acf44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# 9️⃣ Prediction example\n",
    "# ============================\n",
    "# i = random.randint(0, X_val.shape[0]-1)\n",
    "input_patch = X_val[sample_idx].cpu().squeeze().numpy()\n",
    "true_heatmap = y_val[sample_idx].cpu().squeeze().numpy()\n",
    "\n",
    "model_L1L2.eval()\n",
    "with torch.no_grad():\n",
    "    predicted = model_L1L2(X_val[sample_idx].unsqueeze(0)).cpu().squeeze().numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "# plt.subplot(1, 3, 1); plt.imshow(input_patch, cmap='gray'); plt.title(\"Input Patch\")\n",
    "# plt.subplot(1, 3, 2); plt.imshow(true_heatmap, cmap='hot'); plt.title(\"True Heatmap\")\n",
    "plt.subplot(1, 3, 3); plt.imshow(predicted, cmap='hot'); plt.title(\"Predicted Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd647681",
   "metadata": {},
   "source": [
    "still terrible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1832e7",
   "metadata": {},
   "source": [
    "New Structure, MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188ab4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# 7️⃣ Training setup\n",
    "# ============================\n",
    "model_MSE = CNNModel_new().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_MSE.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1508833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Training loop (Microsample)\n",
    "# ============================\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 8\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_MSE.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # Training loop with tqdm\n",
    "    train_iter = tqdm(range(0, len(X_train_micro), batch_size), desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n",
    "    for i in train_iter:\n",
    "        xb = X_train_micro[i:i+batch_size]\n",
    "        yb = y_train_micro[i:i+batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_MSE(xb)\n",
    "        loss = criterion(outputs, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        avg_so_far = train_loss / ((i // batch_size) + 1)\n",
    "        train_iter.set_postfix(loss=avg_so_far)\n",
    "\n",
    "    # Validation loop with tqdm\n",
    "    model_MSE.eval()\n",
    "    val_loss = 0.0\n",
    "    val_iter = tqdm(range(0, len(X_val_micro), batch_size), desc=f\"Epoch {epoch+1}/{epochs} [Val]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for i in val_iter:\n",
    "            xb = X_val_micro[i:i+batch_size]\n",
    "            yb = y_val_micro[i:i+batch_size]\n",
    "            outputs = model_MSE(xb)\n",
    "            loss = criterion(outputs, yb)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            avg_so_far = val_loss / ((i // batch_size) + 1)\n",
    "            val_iter.set_postfix(loss=avg_so_far)\n",
    "    \n",
    "    \n",
    "    avg_train_loss = train_loss / (len(X_train_micro) / batch_size)\n",
    "    avg_val_loss = val_loss / (len(X_val_micro) / batch_size)\n",
    "\n",
    "    train_loss_hist.append(avg_train_loss)\n",
    "    val_loss_hist.append(avg_val_loss)\n",
    "    live_plot()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c10bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# 9️⃣ Prediction example\n",
    "# ============================\n",
    "# i = random.randint(0, X_val.shape[0]-1)\n",
    "input_patch = X_val[sample_idx].cpu().squeeze().numpy()\n",
    "true_heatmap = y_val[sample_idx].cpu().squeeze().numpy()\n",
    "\n",
    "model_MSE.eval()\n",
    "with torch.no_grad():\n",
    "    predicted = model_MSE(X_val[sample_idx].unsqueeze(0)).cpu().squeeze().numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "# plt.subplot(1, 3, 1); plt.imshow(input_patch, cmap='gray'); plt.title(\"Input Patch\")\n",
    "# plt.subplot(1, 3, 2); plt.imshow(true_heatmap, cmap='hot'); plt.title(\"True Heatmap\")\n",
    "plt.subplot(1, 3, 3); plt.imshow(predicted, cmap='hot'); plt.title(\"Predicted Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389f21dc",
   "metadata": {},
   "source": [
    "still terrible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8f90c0",
   "metadata": {},
   "source": [
    "New Structure, BCE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dce794",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# 7️⃣ Training setup\n",
    "# ============================\n",
    "model_BCE = CNNModel_new().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([20.0], device=device))\n",
    "optimizer = optim.Adam(model_BCE.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd9a017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Training loop (Microsample)\n",
    "# ============================\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 8\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_BCE.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # Training loop with tqdm\n",
    "    train_iter = tqdm(range(0, len(X_train_micro), batch_size), desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n",
    "    for i in train_iter:\n",
    "        xb = X_train_micro[i:i+batch_size]\n",
    "        yb = y_train_micro[i:i+batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_BCE(xb)\n",
    "        loss = criterion(outputs, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        avg_so_far = train_loss / ((i // batch_size) + 1)\n",
    "        train_iter.set_postfix(loss=avg_so_far)\n",
    "\n",
    "    # Validation loop with tqdm\n",
    "    model_BCE.eval()\n",
    "    val_loss = 0.0\n",
    "    val_iter = tqdm(range(0, len(X_val_micro), batch_size), desc=f\"Epoch {epoch+1}/{epochs} [Val]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for i in val_iter:\n",
    "            xb = X_val_micro[i:i+batch_size]\n",
    "            yb = y_val_micro[i:i+batch_size]\n",
    "            outputs = model_BCE(xb)\n",
    "            loss = criterion(outputs, yb)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            avg_so_far = val_loss / ((i // batch_size) + 1)\n",
    "            val_iter.set_postfix(loss=avg_so_far)\n",
    "\n",
    "    avg_train_loss = train_loss / (len(X_train_micro) / batch_size)\n",
    "    avg_val_loss = val_loss / (len(X_val_micro) / batch_size)\n",
    "\n",
    "    train_loss_hist.append(avg_train_loss)\n",
    "    val_loss_hist.append(avg_val_loss)\n",
    "    live_plot()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac4ceb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# 9️⃣ Prediction example\n",
    "# ============================\n",
    "# i = random.randint(0, X_val.shape[0]-1)\n",
    "input_patch = X_val[sample_idx].cpu().squeeze().numpy()\n",
    "true_heatmap = y_val[sample_idx].cpu().squeeze().numpy()\n",
    "\n",
    "model_BCE.eval()\n",
    "with torch.no_grad():\n",
    "    predicted = model_BCE(X_val[sample_idx].unsqueeze(0)).cpu().squeeze().numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "# plt.subplot(1, 3, 1); plt.imshow(input_patch, cmap='gray'); plt.title(\"Input Patch\")\n",
    "# plt.subplot(1, 3, 2); plt.imshow(true_heatmap, cmap='hot'); plt.title(\"True Heatmap\")\n",
    "plt.subplot(1, 3, 3); plt.imshow(predicted, cmap='hot'); plt.title(\"Predicted Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d062da",
   "metadata": {},
   "source": [
    "maybe with many many epochs, may converge to a better fit\n",
    "slightly better with new output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42176c1",
   "metadata": {},
   "source": [
    "New Structure, Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0246e5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# 7️⃣ Training setup\n",
    "# ============================\n",
    "model_FL = CNNModel_new().to(device)\n",
    "criterion = FocalLoss(alpha=0.75, gamma=2.0)\n",
    "optimizer = optim.Adam(model_FL.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27228e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Training loop (Microsample)\n",
    "# ============================\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 8\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_FL.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # Training loop with tqdm\n",
    "    train_iter = tqdm(range(0, len(X_train_micro), batch_size), desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n",
    "    for i in train_iter:\n",
    "        xb = X_train_micro[i:i+batch_size]\n",
    "        yb = y_train_micro[i:i+batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_FL(xb)\n",
    "        loss = criterion(yb, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        avg_so_far = train_loss / ((i // batch_size) + 1)\n",
    "        train_iter.set_postfix(loss=avg_so_far)\n",
    "\n",
    "    # Validation loop with tqdm\n",
    "    model_FL.eval()\n",
    "    val_loss = 0.0\n",
    "    val_iter = tqdm(range(0, len(X_val_micro), batch_size), desc=f\"Epoch {epoch+1}/{epochs} [Val]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for i in val_iter:\n",
    "            xb = X_val_micro[i:i+batch_size]\n",
    "            yb = y_val_micro[i:i+batch_size]\n",
    "            outputs = model_FL(xb)\n",
    "            loss = criterion(yb, outputs)\n",
    "            val_loss += loss.item()\n",
    "            avg_so_far = val_loss / ((i // batch_size) + 1)\n",
    "            val_iter.set_postfix(loss=avg_so_far)\n",
    "\n",
    "    avg_train_loss = train_loss / (len(X_train_micro) / batch_size)\n",
    "    avg_val_loss = val_loss / (len(X_val_micro) / batch_size)\n",
    "\n",
    "    train_loss_hist.append(avg_train_loss)\n",
    "    val_loss_hist.append(avg_val_loss)\n",
    "    live_plot()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606c4434",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# 9️⃣ Prediction example\n",
    "# ============================\n",
    "# i = random.randint(0, X_val.shape[0]-1)\n",
    "input_patch = X_val[sample_idx].cpu().squeeze().numpy()\n",
    "true_heatmap = y_val[sample_idx].cpu().squeeze().numpy()\n",
    "\n",
    "model_FL.eval()\n",
    "with torch.no_grad():\n",
    "    predicted = model_FL(X_val[sample_idx].unsqueeze(0)).cpu().squeeze().numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "# plt.subplot(1, 3, 1); plt.imshow(input_patch, cmap='gray'); plt.title(\"Input Patch\")\n",
    "# plt.subplot(1, 3, 2); plt.imshow(true_heatmap, cmap='hot'); plt.title(\"True Heatmap\")\n",
    "plt.subplot(1, 3, 3); plt.imshow(predicted, cmap='hot'); plt.title(\"Predicted Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef1d7fc",
   "metadata": {},
   "source": [
    "Train new model with FocalLoss on full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb45e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# 7️⃣ Training setup\n",
    "# ============================\n",
    "model_FL_Complete = CNNModel_new().to(device)\n",
    "criterion = FocalLoss(alpha=0.75, gamma=2.0)\n",
    "optimizer = optim.Adam(model_FL_Complete.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7a99c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# 8️⃣ Training loop\n",
    "# ============================\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_FL_Complete.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # Training loop with tqdm\n",
    "    train_iter = tqdm(range(0, len(X_train), batch_size), desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n",
    "    for i in train_iter:\n",
    "        xb = X_train[i:i+batch_size]\n",
    "        yb = y_train[i:i+batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_FL_Complete(xb)\n",
    "        loss = criterion(yb, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        avg_so_far = train_loss / ((i // batch_size) + 1)\n",
    "        train_iter.set_postfix(loss=avg_so_far)\n",
    "\n",
    "    # Validation loop with tqdm\n",
    "    model_FL_Complete.eval()\n",
    "    val_loss = 0.0\n",
    "    val_iter = tqdm(range(0, len(X_val), batch_size), desc=f\"Epoch {epoch+1}/{epochs} [Val]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for i in val_iter:\n",
    "            xb = X_val[i:i+batch_size]\n",
    "            yb = y_val[i:i+batch_size]\n",
    "            outputs = model_FL_Complete(xb)\n",
    "            loss = criterion(yb, outputs)\n",
    "            val_loss += loss.item()\n",
    "            avg_so_far = val_loss / ((i // batch_size) + 1)\n",
    "            val_iter.set_postfix(loss=avg_so_far)\n",
    "\n",
    "    avg_train_loss = train_loss / (len(X_train) / batch_size)\n",
    "    avg_val_loss = val_loss / (len(X_val) / batch_size)\n",
    "\n",
    "    train_loss_hist.append(avg_train_loss)\n",
    "    val_loss_hist.append(avg_val_loss)\n",
    "    live_plot()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "#save model\n",
    "torch.save(model_FL_Complete.state_dict(), \"model_FL_Complete.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15782091",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# 9️⃣ Prediction example\n",
    "# ============================\n",
    "# i = random.randint(0, X_val.shape[0]-1)\n",
    "input_patch = X_val[sample_idx].cpu().squeeze().numpy()\n",
    "true_heatmap = y_val[sample_idx].cpu().squeeze().numpy()\n",
    "\n",
    "model_FL_Complete.eval()\n",
    "with torch.no_grad():\n",
    "    predicted = model_FL_Complete(X_val[sample_idx].unsqueeze(0)).cpu().squeeze().numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "# plt.subplot(1, 3, 1); plt.imshow(input_patch, cmap='gray'); plt.title(\"Input Patch\")\n",
    "# plt.subplot(1, 3, 2); plt.imshow(true_heatmap, cmap='hot'); plt.title(\"True Heatmap\")\n",
    "plt.subplot(1, 3, 3); plt.imshow(predicted, cmap='hot'); plt.title(\"Predicted Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959c4e89",
   "metadata": {},
   "source": [
    "compare, old model with FocalLoss on full set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85276ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# 7️⃣ Training setup\n",
    "# ============================\n",
    "model_FL_old_Complete = CNNModel().to(device)\n",
    "criterion = FocalLoss(alpha=0.75, gamma=2.0)\n",
    "optimizer = optim.Adam(model_FL_old_Complete.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404d8325",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# 8️⃣ Training loop\n",
    "# ============================\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_FL_old_Complete.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # Training loop with tqdm\n",
    "    train_iter = tqdm(range(0, len(X_train), batch_size), desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n",
    "    for i in train_iter:\n",
    "        xb = X_train[i:i+batch_size]\n",
    "        yb = y_train[i:i+batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_FL_old_Complete(xb)\n",
    "        loss = criterion(yb, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        avg_so_far = train_loss / ((i // batch_size) + 1)\n",
    "        train_iter.set_postfix(loss=avg_so_far)\n",
    "\n",
    "    # Validation loop with tqdm\n",
    "    model_FL_old_Complete.eval()\n",
    "    val_loss = 0.0\n",
    "    val_iter = tqdm(range(0, len(X_val), batch_size), desc=f\"Epoch {epoch+1}/{epochs} [Val]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for i in val_iter:\n",
    "            xb = X_val[i:i+batch_size]\n",
    "            yb = y_val[i:i+batch_size]\n",
    "            outputs = model_FL_old_Complete(xb)\n",
    "            loss = criterion(yb, outputs)\n",
    "            val_loss += loss.item()\n",
    "            avg_so_far = val_loss / ((i // batch_size) + 1)\n",
    "            val_iter.set_postfix(loss=avg_so_far)\n",
    "\n",
    "    avg_train_loss = train_loss / (len(X_train) / batch_size)\n",
    "    avg_val_loss = val_loss / (len(X_val) / batch_size)\n",
    "\n",
    "    train_loss_hist.append(avg_train_loss)\n",
    "    val_loss_hist.append(avg_val_loss)\n",
    "    live_plot()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "#save model\n",
    "torch.save(model_FL_Complete.state_dict(), 'model_FL_old_Complete.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4fcf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# 9️⃣ Prediction example\n",
    "# ============================\n",
    "# i = random.randint(0, X_val.shape[0]-1)\n",
    "input_patch = X_val[sample_idx].cpu().squeeze().numpy()\n",
    "true_heatmap = y_val[sample_idx].cpu().squeeze().numpy()\n",
    "\n",
    "model_FL_old_Complete.eval()\n",
    "with torch.no_grad():\n",
    "    predicted = model_FL_old_Complete(X_val[sample_idx].unsqueeze(0)).cpu().squeeze().numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "# plt.subplot(1, 3, 1); plt.imshow(input_patch, cmap='gray'); plt.title(\"Input Patch\")\n",
    "# plt.subplot(1, 3, 2); plt.imshow(true_heatmap, cmap='hot'); plt.title(\"True Heatmap\")\n",
    "plt.subplot(1, 3, 3); plt.imshow(predicted, cmap='hot'); plt.title(\"Predicted Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a031249c",
   "metadata": {},
   "source": [
    "seems to perform just as well as without changes to the output layer.........."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52caf4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load models\n",
    "# model_FL_Complete = CNNModel().to(device)\n",
    "# model_FL_old_Complete = CNNModel().to(device)\n",
    "\n",
    "# model_FL_Complete.load_state_dict(torch.load(\"model_FL_Complete.pth\"))\n",
    "# model_FL_old_Complete.load_state_dict(torch.load(\"model_FL_old_Complete.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a61b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from skimage.feature import peak_local_max\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# ============================\n",
    "# 🔍 Utility: extract emitter coordinates\n",
    "# ============================\n",
    "def extract_emitters(heatmap, threshold=0.3, min_distance=2):\n",
    "    \"\"\"\n",
    "    Extract emitter coordinates from a heatmap using non-maximum suppression.\n",
    "    heatmap: 2D numpy array\n",
    "    threshold: minimum relative intensity for detection\n",
    "    min_distance: minimum spacing between peaks\n",
    "    \"\"\"\n",
    "    coords = peak_local_max(\n",
    "        heatmap, \n",
    "        min_distance=min_distance, \n",
    "        threshold_abs=threshold * heatmap.max()\n",
    "    )\n",
    "    return coords  # array of (y, x) coords\n",
    "\n",
    "# ============================\n",
    "# 📐 Metric computation\n",
    "# ============================\n",
    "def evaluate_localization(model, X, Y, radius=2, threshold=0.3, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Evaluate model predictions against ground truth heatmaps.\n",
    "    Returns precision, recall, f1, rmse\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_tp, all_fp, all_fn = 0, 0, 0\n",
    "    localization_errors = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(X)):\n",
    "            xb = X[i].unsqueeze(0).to(device)\n",
    "            yb = Y[i].cpu().squeeze().numpy()\n",
    "\n",
    "            pred = model(xb).cpu().squeeze().numpy()\n",
    "\n",
    "            # Get emitter coordinates\n",
    "            gt_coords = extract_emitters(yb, threshold=0.3, min_distance=2)\n",
    "            pred_coords = extract_emitters(pred, threshold=threshold, min_distance=2)\n",
    "\n",
    "            if len(gt_coords) == 0 and len(pred_coords) == 0:\n",
    "                continue\n",
    "\n",
    "            # Match predictions to ground truth using KDTree\n",
    "            if len(gt_coords) > 0 and len(pred_coords) > 0:\n",
    "                tree = cKDTree(gt_coords)\n",
    "                dists, idxs = tree.query(pred_coords, distance_upper_bound=radius)\n",
    "\n",
    "                # True Positives = matches within radius\n",
    "                tp_mask = dists != np.inf\n",
    "                tp = np.sum(tp_mask)\n",
    "                fp = len(pred_coords) - tp\n",
    "                fn = len(gt_coords) - tp\n",
    "\n",
    "                # Localization error for matched emitters\n",
    "                localization_errors.extend(dists[tp_mask])\n",
    "\n",
    "            else:\n",
    "                tp, fp, fn = 0, len(pred_coords), len(gt_coords)\n",
    "\n",
    "            all_tp += tp\n",
    "            all_fp += fp\n",
    "            all_fn += fn\n",
    "\n",
    "    # Precision, Recall, F1\n",
    "    precision = all_tp / (all_tp + all_fp + 1e-8)\n",
    "    recall    = all_tp / (all_tp + all_fn + 1e-8)\n",
    "    f1        = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    rmse      = np.sqrt(np.mean(np.square(localization_errors))) if localization_errors else np.nan\n",
    "\n",
    "    return precision, recall, f1, rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0314d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f1, rmse = evaluate_localization(\n",
    "    model_FL_old_Complete, \n",
    "    X_val, \n",
    "    y_val, \n",
    "    radius=2,      # tolerance in px\n",
    "    threshold=0.3, # adjust based on your heatmap scaling\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}, RMSE: {rmse:.3f} px\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99b5375",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f1, rmse = evaluate_localization(\n",
    "    model_FL_Complete, \n",
    "    X_val, \n",
    "    y_val, \n",
    "    radius=2,      # tolerance in px\n",
    "    threshold=0.3, # adjust based on your heatmap scaling\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}, RMSE: {rmse:.3f} px\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b8740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# ============================\n",
    "# 🔍 Evaluation functions\n",
    "# ============================\n",
    "def psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float(\"inf\")\n",
    "    max_pixel = 1.0 if img1.max() <= 1.0 else 255.0\n",
    "    return 20 * math.log10(max_pixel / math.sqrt(mse))\n",
    "\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    model.eval()\n",
    "    psnr_list, ssim_list, mse_list = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(X_val)):\n",
    "            xb = X_val[i].unsqueeze(0)\n",
    "            yb = y_val[i].cpu().squeeze().numpy()\n",
    "            pred = model(xb).cpu().squeeze().numpy()\n",
    "\n",
    "            # Normalize predicted output to match target scale\n",
    "            if pred.max() > 0:\n",
    "                pred = pred / pred.max()\n",
    "            if yb.max() > 0:\n",
    "                yb = yb / yb.max()\n",
    "\n",
    "            psnr_list.append(psnr(pred, yb))\n",
    "            ssim_list.append(ssim(pred, yb, data_range=1.0))\n",
    "            mse_list.append(np.mean((pred - yb) ** 2))\n",
    "    return np.mean(psnr_list), np.mean(ssim_list), np.mean(mse_list)\n",
    "\n",
    "# ============================\n",
    "# 📊 Compare all models\n",
    "# ============================\n",
    "models = {\n",
    "    \"model_L1L2_old\": model_L1L2_old,\n",
    "    \"model_MSE_old\": model_MSE_old,\n",
    "    \"model_BCE_old\": model_BCE_old,\n",
    "    \"model_focal_old\": model_focal_old,\n",
    "    \"model_L1L2\": model_L1L2,\n",
    "    \"model_MSE\": model_MSE,\n",
    "    \"model_BCE\": model_BCE,\n",
    "    \"model_FL\": model_FL,\n",
    "    \"model_FL_Complete\": model_FL_Complete,\n",
    "    \"model_FL_old_Complete\": model_FL_old_Complete,\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    psnr_val, ssim_val, mse_val = evaluate_model(model, X_val, y_val)\n",
    "    results[name] = {\"PSNR\": psnr_val, \"SSIM\": ssim_val, \"MSE\": mse_val}\n",
    "\n",
    "# Convert results to a clean table\n",
    "import pandas as pd\n",
    "df_results = pd.DataFrame(results).T\n",
    "print(df_results)\n",
    "\n",
    "# Optional: save results\n",
    "df_results.to_csv(\"model_comparison_metrics.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316989c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load metrics\n",
    "df = pd.read_csv(\"model_comparison_metrics.csv\", index_col=0)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(\n",
    "    df, annot=True, fmt=\".3f\", cmap=\"viridis\", \n",
    "    cbar_kws={'label': 'Metric Value'},\n",
    "    vmax=50  # clip color scaling, e.g. 50\n",
    ")\n",
    "plt.title(\"Model Comparison Heatmap (PSNR, SSIM, MSE)\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.xlabel(\"Metric\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90e7f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load metrics\n",
    "df = pd.read_csv(\"model_comparison_metrics.csv\", index_col=0)\n",
    "# Normalized copy\n",
    "df_norm = df.copy()\n",
    "\n",
    "for col in df.columns:\n",
    "    if col == \"MSE\":\n",
    "        # Exclude BCE models from min/max calculation\n",
    "        mask_valid = ~df.index.str.contains(\"BCE\")\n",
    "        col_min, col_max = df.loc[mask_valid, col].min(), df.loc[mask_valid, col].max()\n",
    "        df_norm.loc[mask_valid, col] = (df.loc[mask_valid, col] - col_min) / (col_max - col_min)\n",
    "        # BCE rows remain unnormalized (will be masked black later)\n",
    "        df_norm.loc[~mask_valid, col] = np.nan\n",
    "    else:\n",
    "        # Normal min-max normalization\n",
    "        col_min, col_max = df[col].min(), df[col].max()\n",
    "        df_norm[col] = (df[col] - col_min) / (col_max - col_min)\n",
    "\n",
    "# Mask for outlier cells (MSE column for BCE models)\n",
    "mask = np.zeros_like(df_norm, dtype=bool)\n",
    "mask[df.index.str.contains(\"BCE\"), df.columns.get_loc(\"MSE\")] = True\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(\n",
    "    df_norm, annot=df, fmt=\".3f\", cmap=\"viridis\",\n",
    "    cbar_kws={'label': 'Normalized Metric Value'},\n",
    "    mask=mask,\n",
    "    linewidths=0.5, linecolor=\"gray\"\n",
    ")\n",
    "\n",
    "# Manually color masked cells black\n",
    "for i in range(len(df)):\n",
    "    for j in range(len(df.columns)):\n",
    "        if mask[i, j]:\n",
    "            plt.gca().add_patch(plt.Rectangle((j, i), 1, 1, fill=True, color='black', ec='gray'))\n",
    "\n",
    "plt.title(\"Model Comparison Heatmap (Normalized per Metric)\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.xlabel(\"Metric\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad13333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Map names in CSV to actual model objects\n",
    "model_dict = {\n",
    "    \"model_L1L2_old\": model_L1L2_old,\n",
    "    \"model_MSE_old\": model_MSE_old,\n",
    "    \"model_BCE_old\": model_BCE_old,\n",
    "    \"model_focal_old\": model_focal_old,\n",
    "    \"model_L1L2\": model_L1L2,\n",
    "    \"model_MSE\": model_MSE,\n",
    "    \"model_BCE\": model_BCE,\n",
    "    \"model_FL\": model_FL,\n",
    "    \"model_FL_Complete\": model_FL_Complete,\n",
    "    \"model_FL_old_Complete\": model_FL_old_Complete,\n",
    "}\n",
    "\n",
    "# Pick test sample\n",
    "# i = 0\n",
    "input_patch = X_val[sample_idx].cpu().squeeze().numpy()\n",
    "true_patch = y_val[sample_idx].cpu().squeeze().numpy()\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# Input\n",
    "plt.subplot(3, 5, 1)\n",
    "plt.imshow(input_patch, cmap=\"gray\")\n",
    "plt.title(\"Input\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Ground truth\n",
    "plt.subplot(3, 5, 2)\n",
    "plt.imshow(true_patch, cmap=\"gray\")\n",
    "plt.title(\"Ground Truth\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Predictions for each model\n",
    "for j, name in enumerate(df.index, start=3):  # order from CSV\n",
    "    model = model_dict[name]\n",
    "    pred = model(X_val[sample_idx].unsqueeze(0)).cpu().squeeze().detach().numpy()\n",
    "\n",
    "    # Optional: add PSNR/SSIM on title from CSV\n",
    "    psnr = df.loc[name, \"PSNR\"]\n",
    "    ssim = df.loc[name, \"SSIM\"]\n",
    "\n",
    "    plt.subplot(3, 5, j)\n",
    "    plt.imshow(pred, cmap=\"gray\")\n",
    "    plt.title(f\"{name}\\nPSNR:{psnr:.2f}, SSIM:{ssim:.3f}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Side-by-Side Reconstructions\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92c2da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# ============================\n",
    "# 🟦 IoU evaluation\n",
    "# ============================\n",
    "def compute_iou(pred, gt, threshold=0.1):\n",
    "    pred_bin = (pred >= threshold).astype(np.uint8)\n",
    "    gt_bin = (gt >= threshold).astype(np.uint8)\n",
    "\n",
    "    intersection = np.logical_and(pred_bin, gt_bin).sum()\n",
    "    union = np.logical_or(pred_bin, gt_bin).sum()\n",
    "\n",
    "    if union == 0:\n",
    "        return 1.0 if intersection == 0 else 0.0\n",
    "    return intersection / union\n",
    "\n",
    "def evaluate_model_iou(model, X_val, y_val, threshold=0.2):\n",
    "    model.eval()\n",
    "    iou_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(X_val)):\n",
    "            xb = X_val[i].unsqueeze(0)\n",
    "            yb = y_val[i].cpu().squeeze().numpy()\n",
    "            pred = model(xb).cpu().squeeze().numpy()\n",
    "\n",
    "            # Normalize predicted output to [0,1]\n",
    "            if pred.max() > 0:\n",
    "                pred = pred / pred.max()\n",
    "            if yb.max() > 0:\n",
    "                yb = yb / yb.max()\n",
    "\n",
    "            iou_list.append(compute_iou(pred, yb, threshold=threshold))\n",
    "    return np.mean(iou_list)\n",
    "\n",
    "# ============================\n",
    "# 📊 IoU for last two models\n",
    "# ============================\n",
    "iou_results = {}\n",
    "models_to_check = {\n",
    "    \"model_FL_Complete\": model_FL_Complete,\n",
    "    \"model_FL_old_Complete\": model_FL_old_Complete,\n",
    "}\n",
    "\n",
    "for name, model in models_to_check.items():\n",
    "    print(f\"Evaluating IoU for {name}...\")\n",
    "    iou_val = evaluate_model_iou(model, X_val, y_val, threshold=0.5)\n",
    "    iou_results[name] = {\"IoU@0.1\": iou_val}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_iou = pd.DataFrame(iou_results).T\n",
    "print(df_iou)\n",
    "\n",
    "# Save separately\n",
    "df_iou.to_csv(\"model_iou_metrics.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aada2a",
   "metadata": {},
   "source": [
    "volin splot/box and whisker plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43c2423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# ============================\n",
    "# 🎻 Collect individual IoU scores for violin plots\n",
    "# ============================\n",
    "def evaluate_model_iou_detailed(model, X_val, y_val, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Same as evaluate_model_iou but returns individual IoU scores instead of mean\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    iou_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(X_val)):\n",
    "            xb = X_val[i].unsqueeze(0)\n",
    "            yb = y_val[i].cpu().squeeze().numpy()\n",
    "            pred = model(xb).cpu().squeeze().numpy()\n",
    "\n",
    "            # Normalize predicted output to [0,1]\n",
    "            if pred.max() > 0:\n",
    "                pred = pred / pred.max()\n",
    "            if yb.max() > 0:\n",
    "                yb = yb / yb.max()\n",
    "\n",
    "            iou_list.append(compute_iou(pred, yb, threshold=threshold))\n",
    "    return iou_list\n",
    "\n",
    "# Collect detailed IoU scores for violin plots\n",
    "print(\"Collecting detailed IoU scores for visualization...\")\n",
    "iou_detailed = {}\n",
    "for name, model in models_to_check.items():\n",
    "    print(f\"Collecting IoU scores for {name}...\")\n",
    "    iou_scores = evaluate_model_iou_detailed(model, X_val, y_val, threshold=0.5)\n",
    "    iou_detailed[name] = iou_scores\n",
    "\n",
    "# ============================\n",
    "# 📊 Create violin plots\n",
    "# ============================\n",
    "# Prepare data for plotting\n",
    "plot_data = []\n",
    "for model_name, scores in iou_detailed.items():\n",
    "    for score in scores:\n",
    "        plot_data.append({\n",
    "            'Model': model_name.replace('model_', '').replace('_Complete', ''),  # Clean names\n",
    "            'IoU Score': score\n",
    "        })\n",
    "\n",
    "df_plot = pd.DataFrame(plot_data)\n",
    "\n",
    "# Create the violin plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(data=df_plot, x='Model', y='IoU Score', palette='Set2')\n",
    "\n",
    "# Overlay box plot for additional statistics\n",
    "sns.boxplot(data=df_plot, x='Model', y='IoU Score', \n",
    "           width=0.3, boxprops=dict(alpha=0.3), \n",
    "           whiskerprops=dict(alpha=0.3),\n",
    "           capprops=dict(alpha=0.3))\n",
    "\n",
    "plt.title('IoU Score Distribution Comparison\\n(Violin Plot with Box Plot Overlay)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.ylabel('IoU Score', fontsize=12)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add mean values as text annotations\n",
    "for i, (model_name, scores) in enumerate(iou_detailed.items()):\n",
    "    mean_iou = np.mean(scores)\n",
    "    clean_name = model_name.replace('model_', '').replace('_Complete', '')\n",
    "    plt.text(i, mean_iou + 0.02, f'μ={mean_iou:.3f}', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('iou_violin_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============================\n",
    "# 📈 Additional statistical summary\n",
    "# ============================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"📊 DETAILED IoU STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "stats_summary = {}\n",
    "for model_name, scores in iou_detailed.items():\n",
    "    clean_name = model_name.replace('model_', '').replace('_Complete', '')\n",
    "    stats = {\n",
    "        'Mean': np.mean(scores),\n",
    "        'Median': np.median(scores),\n",
    "        'Std': np.std(scores),\n",
    "        'Min': np.min(scores),\n",
    "        'Max': np.max(scores),\n",
    "        'Q25': np.percentile(scores, 25),\n",
    "        'Q75': np.percentile(scores, 75)\n",
    "    }\n",
    "    stats_summary[clean_name] = stats\n",
    "    \n",
    "    print(f\"\\n{clean_name}:\")\n",
    "    print(f\"  Mean: {stats['Mean']:.4f}\")\n",
    "    print(f\"  Median: {stats['Median']:.4f}\")\n",
    "    print(f\"  Std: {stats['Std']:.4f}\")\n",
    "    print(f\"  Range: [{stats['Min']:.4f}, {stats['Max']:.4f}]\")\n",
    "    print(f\"  IQR: [{stats['Q25']:.4f}, {stats['Q75']:.4f}]\")\n",
    "\n",
    "# Convert stats to DataFrame and save\n",
    "df_stats = pd.DataFrame(stats_summary).T\n",
    "df_stats.to_csv(\"model_iou_detailed_stats.csv\")\n",
    "print(f\"\\nDetailed statistics saved to 'model_iou_detailed_stats.csv'\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632c8cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_random_iou_samples(model, X_val, y_val, n_samples=3, threshold=0.5):\n",
    "    model.eval()\n",
    "    idxs = random.sample(range(len(X_val)), n_samples)\n",
    "\n",
    "    fig, axes = plt.subplots(n_samples, 3, figsize=(10, 4 * n_samples))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for row, i in enumerate(idxs):\n",
    "            xb = X_val[i].unsqueeze(0)\n",
    "            yb = y_val[i].cpu().squeeze().numpy()\n",
    "            pred = model(xb).cpu().squeeze().numpy()\n",
    "\n",
    "            # Normalize\n",
    "            if pred.max() > 0:\n",
    "                pred = pred / pred.max()\n",
    "            if yb.max() > 0:\n",
    "                yb = yb / yb.max()\n",
    "\n",
    "            # Binarize\n",
    "            pred_bin = (pred >= threshold).astype(np.uint8)\n",
    "            gt_bin = (yb >= threshold).astype(np.uint8)\n",
    "\n",
    "            # Compute IoU\n",
    "            iou_val = compute_iou(pred, yb, threshold=threshold)\n",
    "\n",
    "            # Plot GT, Pred, Overlay\n",
    "            axes[row, 0].imshow(gt_bin, cmap=\"gray\")\n",
    "            axes[row, 0].set_title(\"Ground Truth\")\n",
    "            axes[row, 0].axis(\"off\")\n",
    "\n",
    "            axes[row, 1].imshow(pred_bin, cmap=\"gray\")\n",
    "            axes[row, 1].set_title(\"Prediction\")\n",
    "            axes[row, 1].axis(\"off\")\n",
    "\n",
    "            axes[row, 2].imshow(gt_bin, cmap=\"gray\", alpha=0.5, label=\"GT\")\n",
    "            axes[row, 2].imshow(pred_bin, cmap=\"Reds\", alpha=0.5, label=\"Pred\")\n",
    "            axes[row, 2].set_title(f\"Overlay\\nIoU={iou_val:.3f}\")\n",
    "            axes[row, 2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6753f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_random_iou_samples(model_FL_Complete, X_val, y_val, n_samples=3, threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44e6503",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_random_iou_samples(model_FL_old_Complete, X_val, y_val, n_samples=3, threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cab7940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TRP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
